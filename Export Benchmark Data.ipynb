{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0236985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbaed909",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://172.20.10.2:9999/blazegraph/sparql\" # SPARQL endpoint hosting ITO.owl\n",
    "prefixes = \"\"\"\n",
    "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix ito: <https://identifiers.org/ito:>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\"\"\"\n",
    "\n",
    "def query(service, query, numeric_cols = []):\n",
    "    \"\"\"\n",
    "    Helper function to convert SPARQL results into a Pandas data frame.\n",
    "    \"\"\"\n",
    "    sparql = SPARQLWrapper(service)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    result = sparql.query()\n",
    "    processed_results = json.load(result.response)\n",
    "    cols = processed_results['head']['vars']\n",
    "\n",
    "    out = []\n",
    "    for row in processed_results['results']['bindings']:\n",
    "        item = []\n",
    "        for c in cols:\n",
    "            item.append(row.get(c, {}).get('value'))\n",
    "        out.append(item)\n",
    "        \n",
    "    df = pd.DataFrame(out, columns=cols)\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12168971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>performance_measure_label</th>\n",
       "      <th>result</th>\n",
       "      <th>date</th>\n",
       "      <th>benchmark_dataset_label</th>\n",
       "      <th>benchmark_label</th>\n",
       "      <th>individual_process_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Error Rate</td>\n",
       "      <td>5.09</td>\n",
       "      <td>2019-10-15</td>\n",
       "      <td>COFW dataset</td>\n",
       "      <td>Benchmarking</td>\n",
       "      <td>CHR2C (Inter-pupils Norm) model in \\'Cascade o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mean Error Rate</td>\n",
       "      <td>5.27</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>COFW dataset</td>\n",
       "      <td>Benchmarking</td>\n",
       "      <td>DCFE model in \\'A Deeply-initialized Coarse-to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mean Error Rate</td>\n",
       "      <td>5.04</td>\n",
       "      <td>2022-02-04</td>\n",
       "      <td>COFW dataset</td>\n",
       "      <td>Benchmarking</td>\n",
       "      <td>MNN+OR (Inter-pupils Norm) model in \\'Multi-ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mean Error Rate</td>\n",
       "      <td>5.11</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>COFW dataset</td>\n",
       "      <td>Benchmarking</td>\n",
       "      <td>3DDE (Inter-pupil Norm) model in \\'Face Alignm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mean Error Rate</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>COFW dataset</td>\n",
       "      <td>Benchmarking</td>\n",
       "      <td>SH-FAN model in \\'Subpixel Heatmap Regression ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  performance_measure_label result        date benchmark_dataset_label  \\\n",
       "0           Mean Error Rate   5.09  2019-10-15            COFW dataset   \n",
       "1           Mean Error Rate   5.27  2018-09-01            COFW dataset   \n",
       "2           Mean Error Rate   5.04  2022-02-04            COFW dataset   \n",
       "3           Mean Error Rate   5.11  2019-02-05            COFW dataset   \n",
       "4           Mean Error Rate   3.02  2021-11-03            COFW dataset   \n",
       "\n",
       "  benchmark_label                           individual_process_label  \n",
       "0    Benchmarking  CHR2C (Inter-pupils Norm) model in \\'Cascade o...  \n",
       "1    Benchmarking  DCFE model in \\'A Deeply-initialized Coarse-to...  \n",
       "2    Benchmarking  MNN+OR (Inter-pupils Norm) model in \\'Multi-ta...  \n",
       "3    Benchmarking  3DDE (Inter-pupil Norm) model in \\'Face Alignm...  \n",
       "4    Benchmarking  SH-FAN model in \\'Subpixel Heatmap Regression ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "prefix ito: <https://identifiers.org/ito:>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "\n",
    "select ?performance_measure_label ?result ?date ?benchmark_dataset_label ?benchmark_label ?individual_process_label\n",
    "where {\n",
    "    ?performance_measure rdfs:subPropertyOf* ito:performance_measure .\n",
    "    ?performance_measure rdfs:label ?performance_measure_label .\n",
    "    ?benchmark_process_individual ?performance_measure ?result .\n",
    "    ?benchmark_process_individual <http://www.geneontology.org/formats/oboInOwl#date> ?date .\n",
    "    ?benchmark_process_individual ito:has_input ?benchmark_dataset .\n",
    "    ?benchmark_dataset rdfs:label ?benchmark_dataset_label .\n",
    "    ?benchmark_process_individual a ?benchmark_process .\n",
    "    ?benchmark_process rdfs:subClassOf* ?high_level_process .\n",
    "    ?high_level_process rdfs:subClassOf ito:ITO_01625 .\n",
    "    ?high_level_process rdfs:label ?benchmark_label .\n",
    "    ?benchmark_process_individual rdfs:label ?individual_process_label .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "df = query(endpoint, q)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4cadf64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for future reference; query is somewhat slow\n",
    "df.to_csv('benchmark_dataset_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0d1c90b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1134511\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('benchmark_dataset_raw.csv')\n",
    "\n",
    "# Any duplicated entries?\n",
    "\n",
    "print(df[['date','result','individual_process_label']].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "39b66b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove future benchmarks; dataset was last updated in September 2022\n",
    "\n",
    "df = df[df['date']<= '2022-10-01']\n",
    "\n",
    "# Remove \"Benchmarking\"\n",
    "\n",
    "df = df[df['benchmark_label'] != 'Benchmarking']\n",
    "\n",
    "# Keep only most common performance measure labels\n",
    "\n",
    "df['pm_counts'] = df.groupby('performance_measure_label')['performance_measure_label'].transform(lambda x: x.count())\n",
    "df = df.sort_values(by='pm_counts',ascending=False).groupby(['date','result','individual_process_label']).nth(0).reset_index().drop(columns='Unnamed: 0')\n",
    "df.to_csv('benchmark_dataset_deduplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35cbcf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Any duplicated entries?\n",
    "\n",
    "print(df[['date','result','individual_process_label']].duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1bcf96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy              5058\n",
       "Score                 2282\n",
       "F1                    1488\n",
       "PSNR                  1350\n",
       "Top 1 Accuracy        1079\n",
       "mIoU                   993\n",
       "SSIM                   886\n",
       "mAP                    880\n",
       "Number of params       748\n",
       "FID                    695\n",
       "Top-1 Accuracy         671\n",
       "AP                     637\n",
       "Percentage correct     525\n",
       "AUC                    520\n",
       "Rank-1                 459\n",
       "Top 5 Accuracy         457\n",
       "Mean IoU               457\n",
       "MAE                    455\n",
       "MAP                    442\n",
       "AP50                   425\n",
       "box AP                 421\n",
       "AP75                   391\n",
       "MRR                    360\n",
       "Precision              320\n",
       "GFLOPs                 319\n",
       "ROUGE-L                315\n",
       "Average MPJPE (mm)     315\n",
       "APS                    308\n",
       "BLEU                   301\n",
       "APL                    298\n",
       "Recall                 297\n",
       "APM                    296\n",
       "ROUGE-1                288\n",
       "EM                     280\n",
       "BLEU score             273\n",
       "RMSE                   259\n",
       "Hits-at-10             256\n",
       "Percentage error       256\n",
       "NMI                    255\n",
       "ROUGE-2                250\n",
       "Accuracy (%)           243\n",
       "Validation mIoU        236\n",
       "S-Measure              235\n",
       "Hits-at-1              234\n",
       "Average Accuracy       234\n",
       "R-at-1                 229\n",
       "F1 score               226\n",
       "Top-5 Accuracy         217\n",
       "J&F                    212\n",
       "Jaccard (Mean)         212\n",
       "Name: performance_measure_label, dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What metrics are in the dataset?\n",
    "\n",
    "df['performance_measure_label'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_maxima = {\n",
    "    'accuracy' : 1,\n",
    "    'AUC' : 1,\n",
    "    'Precision' : 1,\n",
    "    'Recall' : 1,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8a2af2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57515</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57516</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57517</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57518</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57519</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57520 rows × 0 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
       "\n",
       "[57520 rows x 0 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anotate with max metrics if appliccable\n",
    "\n",
    "df.loc[df['performance_measure_label'].str.contains(\"ccuracy\"),'peformance_measure_label'] = 'accuracy'\n",
    "\n",
    "\n",
    "\n",
    "df.filter(~(df['performance_measure_label'].str.contains(\"ccuracy\") & (df['result'].astype(float) > 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e9626ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy              5058\n",
       "Score                 2282\n",
       "F1                    1488\n",
       "PSNR                  1350\n",
       "Top 1 Accuracy        1079\n",
       "mIoU                   993\n",
       "SSIM                   886\n",
       "mAP                    880\n",
       "Number of params       748\n",
       "FID                    695\n",
       "Top-1 Accuracy         671\n",
       "AP                     637\n",
       "Percentage correct     525\n",
       "AUC                    520\n",
       "Rank-1                 459\n",
       "Top 5 Accuracy         457\n",
       "Mean IoU               457\n",
       "MAE                    455\n",
       "MAP                    442\n",
       "AP50                   425\n",
       "box AP                 421\n",
       "AP75                   391\n",
       "MRR                    360\n",
       "Precision              320\n",
       "GFLOPs                 319\n",
       "ROUGE-L                315\n",
       "Average MPJPE (mm)     315\n",
       "APS                    308\n",
       "BLEU                   301\n",
       "APL                    298\n",
       "Recall                 297\n",
       "APM                    296\n",
       "ROUGE-1                288\n",
       "EM                     280\n",
       "BLEU score             273\n",
       "RMSE                   259\n",
       "Hits-at-10             256\n",
       "Percentage error       256\n",
       "NMI                    255\n",
       "ROUGE-2                250\n",
       "Accuracy (%)           243\n",
       "Validation mIoU        236\n",
       "S-Measure              235\n",
       "Hits-at-1              234\n",
       "Average Accuracy       234\n",
       "R-at-1                 229\n",
       "F1 score               226\n",
       "Top-5 Accuracy         217\n",
       "J&F                    212\n",
       "Jaccard (Mean)         212\n",
       "Name: performance_measure_label, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a222da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result']=df['result'].astype(float)\n",
    "# Convert % accuracy to 0-1 accuracy\n",
    "inds = (df['performance_measure_label']=='Accuracy') & (df['result'].astype(float) > 1)\n",
    "\n",
    "df.loc[inds,'result']/=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9566f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "indstop1 = (df['performance_measure_label'].isin('Top 1 Accuracy','Top-1 Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
